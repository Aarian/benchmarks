# Generated 2023-10-11 from:
# /home/arian/PycharmProjects/SpeechBrain_Ultra/hparam/NeuralNets_Hparams.yaml
# yamllint disable
#Some Hyper Params
base_channels: 32
kernel_size: 11
CHANNEL_NUM: 256
DIM_CNN_MLP: 277
LR: 0.001
batch_size: 16

#Initial Settings

seed: 12345
__set_seed: !apply:torch.manual_seed [12345]
output_folder: Results/Ultrasound
save_folder: Results/Ultrasound/save
train_log: Results/Ultrasound/train_log.txt
json_folder: ./json_folder

#padding: !ref <kernel_size> // 2

# Data files
train_json: ./json_folder/train.json
valid_json: ./json_folder/valid.json
test_json: ./json_folder/test.json


sorting: ascending


train_dataloader_opts:
  batch_size: 16
  shuffle: true

valid_dataloader_opts:
  batch_size: 16

test_dataloader_opts:
  batch_size: 16

# # CNN Neural Network

# layer1: !new:speechbrain.nnet.CNN.Conv1d #!new:torch.nn.Conv1d
#   in_channels: 1
#   out_channels: !ref <CHANNEL_NUM>
#   kernel_size: 4
#   stride: 2
#   #padding: 1

# layer2: !new:speechbrain.nnet.CNN.Conv1d #!new:torch.nn.Conv1d
#   in_channels: !ref <CHANNEL_NUM>
#   out_channels: !ref <CHANNEL_NUM>//2
#   kernel_size: 4
#   stride: 2
#   #padding: 1
#   bias: False

# layer3: !new:speechbrain.nnet.CNN.Conv1d #!new:torch.nn.Conv1d
#   in_channels: !ref <CHANNEL_NUM>//2
#   out_channels: !ref <CHANNEL_NUM>//4
#   kernel_size: 4
#   stride: 2
#   #padding: 1
#   bias: False

# layer4: !new:speechbrain.nnet.CNN.Conv1d #!new:torch.nn.Conv1d
#   in_channels: !ref <CHANNEL_NUM>//4
#   out_channels: !ref <CHANNEL_NUM>//8
#   kernel_size: 3
#   stride: 1
#   #padding: 1
#   bias: False

# layer5: !new:speechbrain.nnet.CNN.Conv1d #!new:torch.nn.Conv1d
#   in_channels: !ref <CHANNEL_NUM>//8
#   out_channels: 1
#   kernel_size: 4
#   stride: 1
#   #padding: 0
#   bias: False

layer1: &id002 !new:torch.nn.Conv1d
  in_channels: 1
  out_channels: 256
  kernel_size: 4
  stride: 2
  padding: 1

layer2: &id004 !new:torch.nn.Conv1d
  in_channels: 256
  out_channels: 128
  kernel_size: 4
  stride: 2
  padding: 1
  bias: false

layer3: &id007 !new:torch.nn.Conv1d
  in_channels: 128
  out_channels: 64
  kernel_size: 4
  stride: 2
  padding: 1
  bias: false

layer4: &id009 !new:torch.nn.Conv1d
  in_channels: 64
  out_channels: 32
  kernel_size: 3
  stride: 1
  padding: 1
  bias: false

layer5: &id013 !new:torch.nn.Conv1d
  in_channels: 32
  out_channels: 1
  kernel_size: 4
  stride: 1
  padding: 0
  bias: false


Relue: &id003 !new:torch.nn.ReLU
  inplace: true

LeakyRelue: &id006 !new:torch.nn.LeakyReLU
  inplace: true

Batch_norm_1d_layer_2: &id005 !new:torch.nn.BatchNorm1d
  num_features: 128

Batch_norm_1d_layer_3: &id008 !new:torch.nn.BatchNorm1d
  num_features: 64

Batch_norm_1d_layer_4: &id010 !new:torch.nn.BatchNorm1d
  num_features: 32

Avg_pooling_1d: &id011 !new:torch.nn.AvgPool1d
  kernel_size: 3
  stride: 2

Dropout_1d: &id012 !new:torch.nn.Dropout1d
  p: 0.5

LayerNorm: &id014 !new:torch.nn.LayerNorm
  normalized_shape: 277

flatten_L: &id019 !new:torch.nn.Flatten

MLP_L1: &id015 !new:torch.nn.Linear
  in_features: 277
  out_features: 16

MLP_L2: &id016 !new:torch.nn.Linear
  in_features: 16
  out_features: 8

MLP_L3: &id017 !new:torch.nn.Linear
  in_features: 8
  out_features: 1


# Optimizer
optim: &id001 !name:torch.optim.Adam
  lr: 0.001

scheduler: !name:torch.optim.lr_scheduler.LinearLR
  optimizer: *id001
  start_factor: 1.0
  end_factor: 0.5
  total_iters: 30


# Loss function

loss: !new:torch.nn.MSELoss

#Modules

CnnBlock: &id018 !new:torch.nn.Sequential
- *id002
- *id003
- *id004
- *id005
- *id006
- *id007
- *id008
- *id003
- *id009
- *id010
- *id003
- *id011
- *id012
- *id013
- *id006
- *id014
MLPBlock: &id020 !new:torch.nn.Sequential


# Model




# model: !new:torch.nn.Sequential
#   - !ref <layer1>
#   - !ref <Relue>
#   - !ref <layer2>
#   - !ref <Batch_norm_1d_layer_2>
#   - !ref <LeakyRelue>
#   - !ref <layer3>
#   - !ref <Batch_norm_1d_layer_3>
#   - !ref <Relue>
#   - !ref <layer4>
#   - !ref <Batch_norm_1d_layer_4>
#   - !ref <Relue>
#   - !ref <Avg_pooling_1d>
#   - !ref <Dropout_1d>
#   - !ref <layer5>
#   - !ref <LeakyRelue>
#   - !ref <LayerNorm>
#   - !new:torch.nn.Flatten
#   - !ref <MLP_L1>
#   - !ref <LeakyRelue>
#   - !ref <MLP_L2>
#   - !ref <LeakyRelue>
#   - !ref <MLP_L3>


- *id015
- *id006
- *id016
- *id006
- *id017
modules:
  CnnBlock: *id018
  flatten_L: *id019
  MLPBlock: *id020
model1: !new:torch.nn.ModuleList
- [*id018, *id019, *id020]
